# app/providers/text_provider.py (v11.0 ç»ˆæç‰ˆ - å¥å£®çš„å¢é‡è½¬æ¢)

import httpx
import json
import uuid
import time
import traceback
import asyncio
from typing import Dict, Any, AsyncGenerator, Union, List

from fastapi import Request
from fastapi.responses import StreamingResponse, JSONResponse

from app.providers.base import BaseProvider
from app.core.config import settings

import logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class TextProvider(BaseProvider):
    """
    é€šä¹‰åƒé—®å…¨èƒ½æä¾›å•† (v11.0 ç»ˆæç‰ˆ)
    - é‡‡ç”¨æœ€ç»ˆä¿®æ­£çš„çŠ¶æ€åŒ–è§£æå™¨ï¼Œé€»è¾‘æ¸…æ™°ï¼Œç¡®ä¿å°†å®˜ç½‘çš„â€œå…¨é‡ç´¯ç§¯æµâ€æ­£ç¡®è½¬æ¢ä¸ºå®¢æˆ·ç«¯æ‰€éœ€çš„â€œå¢é‡æµâ€ã€‚
    - å½»åº•è§£å†³æ‰€æœ‰åœºæ™¯ä¸‹çš„å†…å®¹é‡å¤é—®é¢˜ã€‚
    - ä¿ç•™äº†ä¼šè¯é¢„çƒ­å’Œå›½é™…ç«™åŠŸèƒ½ã€‚
    """

    # --------------------------------------------------------------------------
    # æ ¸å¿ƒå…¥å£
    # --------------------------------------------------------------------------
    async def chat_completion(self, request_data: Dict[str, Any], original_request: Request) -> Union[StreamingResponse, JSONResponse]:
        model_name = request_data.get("model", "qwen-plus")
        task_type = self._get_task_type(model_name, request_data)
        try:
            if task_type in ["image", "video"]:
                logger.info(f"æ£€æµ‹åˆ° '{task_type}' ä»»åŠ¡ï¼Œå¼ºåˆ¶ä½¿ç”¨å›½é™…ç«™(INTL)æ¨¡å¼...")
                return await self._handle_long_polling_task(request_data)
            else:
                account_id = settings.MODEL_TO_ACCOUNT_MAP.get(model_name, 1)
                logger.info(f"æ£€æµ‹åˆ°æ¨¡å‹ '{model_name}'ï¼Œä»»åŠ¡ç±»å‹ '{task_type}'ï¼Œå°†ä½¿ç”¨å›½å†…ç«™è´¦å· {account_id}...")
                return await self._handle_stream_task(request_data, account_id)
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {type(e).__name__}: {e}")
            traceback.print_exc()
            return JSONResponse(content={"error": {"message": f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}", "type": "provider_error"}}, status_code=500)

    # --------------------------------------------------------------------------
    # å›½å†…ç«™æµå¼ä»»åŠ¡å¤„ç†
    # --------------------------------------------------------------------------
    async def _handle_stream_task(self, request_data: Dict[str, Any], account_id: int) -> StreamingResponse:
        headers = self._prepare_cn_headers(account_id)
        await self._prewarm_session(headers)
        payload = self._prepare_cn_payload(request_data)
        model_name_for_client = request_data.get("model", "qwen-plus")
        url = "https://api.tongyi.com/dialog/conversation"
        logger.info(f"   [CN-Account-{account_id}] æ­£åœ¨å‘æ¨¡å‹ '{model_name_for_client}' å‘é€æµå¼è¯·æ±‚...")
        return StreamingResponse(self._stream_generator(url, headers, payload, model_name_for_client), media_type="text/event-stream")

    async def _prewarm_session(self, headers: Dict[str, Any]):
        try:
            logger.info("   [Pre-warm] æ­£åœ¨å‘é€ä¼šè¯é¢„çƒ­è¯·æ±‚...")
            url = "https://api.tongyi.com/assistant/api/record/list"
            payload = {
                "pageNo": 1, "terminal": "web", "pageSize": 10000, "module": "uploadhistory",
                "fileTypes": ["file", "audio", "video"], "recordSources": ["chat", "zhiwen", "tingwu"],
                "status": [20, 30, 40, 41], "taskTypes": ["local", "net_source", "doc_read", "paper_read", "book_read"]
            }
            async with httpx.AsyncClient() as client:
                prewarm_headers = headers.copy()
                prewarm_headers['Accept'] = 'application/json, text/plain, */*'
                response = await client.post(url, headers=prewarm_headers, json=payload, timeout=10)
                response.raise_for_status()
                logger.info("   [Pre-warm] âœ… ä¼šè¯é¢„çƒ­æˆåŠŸï¼")
        except Exception as e:
            logger.warning(f"   [Pre-warm] âš ï¸ ä¼šè¯é¢„çƒ­å¤±è´¥: {e}ã€‚ç»§ç»­å°è¯•...")

    # --------------------------------------------------------------------------
    # ç»ˆæç‰ˆé«˜çº§æµå¼è§£æå™¨ (v11.0) - æ ¸å¿ƒä¿®æ”¹
    # --------------------------------------------------------------------------
    async def _stream_generator(self, url: str, headers: Dict, payload: Dict, model_name: str) -> AsyncGenerator[str, None]:
        """
        å¥å£®çš„çŠ¶æ€åŒ–æµå¼ç”Ÿæˆå™¨ï¼Œå°†é€šä¹‰åƒé—®çš„â€œå…¨é‡ç´¯ç§¯æµâ€è½¬æ¢ä¸ºæ ‡å‡†çš„â€œå¢é‡æµâ€ã€‚
        """
        chat_id = f"chatcmpl-{uuid.uuid4().hex}"
        is_first_chunk = True
        full_content_so_far = "" # å…³é”®(1) ğŸ§ : çŠ¶æ€å˜é‡åœ¨ç”Ÿæˆå™¨å‡½æ•°çš„é¡¶å±‚ä½œç”¨åŸŸï¼Œç¡®ä¿åœ¨å¾ªç¯ä¸­æŒä¹…å­˜åœ¨

        try:
            async with httpx.AsyncClient(timeout=60) as client:
                async with client.stream("POST", url, headers=headers, json=payload) as response:
                    response.raise_for_status()
                    
                    async for line in response.aiter_lines():
                        if not line.startswith('data:'):
                            continue
                        
                        raw_data_str = line.strip()[len('data:'):]
                        if not raw_data_str or "[DONE]" in raw_data_str:
                            continue
                        
                        try:
                            qwen_data = json.loads(raw_data_str)
                            
                            # ä»æ‰€æœ‰å†…å®¹å—ä¸­ç­›é€‰å‡º 'text' ç±»å‹çš„å†…å®¹å—
                            text_blocks = [block for block in qwen_data.get("contents", []) if block.get("contentType") == "text"]
                            if not text_blocks:
                                continue

                            # é€šå¸¸æˆ‘ä»¬åªå…³å¿ƒæœ€åä¸€ä¸ªtextå—ï¼Œå› ä¸ºå®ƒåŒ…å«äº†æœ€æ–°çš„å®Œæ•´å†…å®¹
                            latest_text_block = text_blocks[-1]
                            new_full_content = latest_text_block.get("content", "")
                            
                            if new_full_content is None:
                                continue

                            # å…³é”®(2) ğŸ’¡: åŸºäºæŒä¹…åŒ–çš„çŠ¶æ€ï¼Œç²¾ç¡®è®¡ç®—å¢é‡
                            delta_content = ""
                            if new_full_content.startswith(full_content_so_far):
                                delta_content = new_full_content[len(full_content_so_far):]
                            else:
                                logger.warning(f"   [Stream Reset] æµå†…å®¹ä¸è¿ç»­ï¼Œå°†å‘é€å…¨éƒ¨æ–°å†…å®¹ã€‚")
                                delta_content = new_full_content

                            # å¦‚æœæ²¡æœ‰å®é™…çš„æ–°å†…å®¹ï¼Œå°±è·³è¿‡
                            if not delta_content:
                                continue

                            # å…³é”®(3) âœ…: å‘é€å¢é‡å‰ï¼Œå…ˆå‘é€è§’è‰²ä¿¡æ¯ï¼ˆä»…ä¸€æ¬¡ï¼‰
                            if is_first_chunk:
                                role_chunk = {
                                    "id": chat_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model_name,
                                    "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}]
                                }
                                yield f"data: {json.dumps(role_chunk, ensure_ascii=False)}\n\n"
                                is_first_chunk = False

                            # å‘é€çœŸæ­£çš„å¢é‡å†…å®¹å—
                            openai_chunk = {
                                "id": chat_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model_name,
                                "choices": [{"index": 0, "delta": {"content": delta_content}, "finish_reason": None}]
                            }
                            yield f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"
                            
                            # å…³é”®(4) ğŸ”„: æ›´æ–°çŠ¶æ€ï¼Œä¸ºå¤„ç†ä¸‹ä¸€ä¸ª data: æ¶ˆæ¯åšå‡†å¤‡
                            full_content_so_far = new_full_content
                                
                        except json.JSONDecodeError:
                            logger.warning(f"   [Warning] JSON è§£æå¤±è´¥: {raw_data_str}")
                            continue
            
            # æµç»“æŸï¼Œå‘é€ç»ˆæ­¢å—
            final_chunk = {
                "id": chat_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model_name,
                "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
            }
            yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
        
        except Exception as e:
            logger.error(f"   [Error] æµå¼ç”Ÿæˆå™¨å‘ç”Ÿé”™è¯¯: {e}")
            traceback.print_exc()
        
        finally:
            logger.info("   [Stream] æµå¼ä¼ è¾“ç»“æŸã€‚")
            yield "data: [DONE]\n\n"

    # --------------------------------------------------------------------------
    # è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜)
    # --------------------------------------------------------------------------
    def _get_task_type(self, model_name: str, request_data: Dict[str, Any]) -> str:
        model_name_lower = model_name.lower()
        if "wanx" in model_name_lower: return "image"
        if "animate" in model_name_lower: return "video"
        if "vl" in model_name_lower or "qvq" in model_name_lower: return "vision"
        return "text"

    def _prepare_cn_headers(self, account_id: int) -> Dict[str, str]:
        try:
            cookie = getattr(settings, f"CN_ACCOUNT_{account_id}_COOKIE")
            xsrf_token = getattr(settings, f"CN_ACCOUNT_{account_id}_XSRF_TOKEN")
        except AttributeError: raise ValueError(f"å›½å†…ç«™è´¦å· {account_id} çš„é…ç½®ä¸å®Œæ•´ã€‚")
        if not cookie or not xsrf_token: raise ValueError(f"å›½å†…ç«™è´¦å· {account_id} çš„è®¤è¯ä¿¡æ¯ä¸ºç©ºã€‚")
        safe_cookie = cookie.encode('utf-8').decode('latin-1')
        return {'Origin': 'https://www.tongyi.com', 'Referer': 'https://www.tongyi.com/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36', 'Cookie': safe_cookie, 'x-xsrf-token': xsrf_token, 'x-platform': 'pc_tongyi', 'Accept': 'text/event-stream', 'Content-Type': 'application/json;charset=UTF-8'}

    def _prepare_cn_payload(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        messages: List[Dict[str, Any]] = request_data.get("messages", [])
        if not messages: messages = [{"role": "user", "content": request_data.get("prompt", "ä½ å¥½")}]
        qwen_contents = []
        for msg in messages:
            content = msg.get("content")
            if isinstance(content, str): qwen_contents.append({"role": msg.get("role"), "content": content, "contentType": "text"})
        model_in_payload = request_data.get("model", "")
        return {"action": "next", "contents": qwen_contents, "model": model_in_payload, "parentMsgId": "", "requestId": str(uuid.uuid4()), "sessionId": "", "sessionType": "text_chat", "userAction": "new_top", "feature_config": {"search_enabled": False, "thinking_enabled": False}}

    # --------------------------------------------------------------------------
    # å›½é™…ç«™ç›¸å…³å‡½æ•° (ä¿ç•™)
    # --------------------------------------------------------------------------
    def _prepare_intl_headers(self) -> Dict[str, str]:
        if not settings.INTL_AUTHORIZATION or not settings.INTL_COOKIE or not settings.INTL_BX_UA:
            raise ValueError("å›½é™…ç«™(intl)è®¤è¯ä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶ã€‚")
        safe_cookie = settings.INTL_COOKIE.encode('utf-8').decode('latin-1')
        return {'Origin': 'https://chat.qwen.ai', 'Referer': 'https://chat.qwen.ai/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36', 'Authorization': settings.INTL_AUTHORIZATION, 'Cookie': safe_cookie, 'bx-ua': settings.INTL_BX_UA}

    async def _handle_long_polling_task(self, request_data: Dict[str, Any]) -> JSONResponse:
        headers = self._prepare_intl_headers()
        headers['Accept'] = 'application/json, text/event-stream'
        headers['Content-Type'] = 'application/json;charset=UTF-8'
        completions_url = "https://chat.qwen.ai/api/v2/chat/completions"
        task_status_url_template = "https://chat.qwen.ai/api/v1/tasks/status/{task_id}"
        prompt = request_data.get("prompt", "ä¸€åªçŒ«")
        model_name = "wanx-v1" if "wanx" in request_data.get("model", "") else "animate-v1"
        msg_type = "t2i" if model_name == "wanx-v1" else "t2v"
        payload = {"action": "next", "contents": [{"content": prompt, "contentType": "text", "role": "user"}], "msg_type": msg_type, "mode": "chat", "model": model_name, "parentMsgId": "", "requestId": str(uuid.uuid4())}
        async with httpx.AsyncClient(timeout=60) as client:
            logger.info(f"   [INTL] æ­£åœ¨å¯åŠ¨ '{model_name}' ä»»åŠ¡...")
            response = await client.post(completions_url, headers=headers, json=payload)
            response.raise_for_status()
            task_id = None
            async for line in response.aiter_lines():
                if line.startswith('data:'):
                    try:
                        data = json.loads(line[len('data:'):])
                        if data.get("taskIds"):
                            task_id = data["taskIds"][0]
                            break
                    except json.JSONDecodeError: continue
            if not task_id: raise ValueError(f"{model_name} ä»»åŠ¡å¯åŠ¨å¤±è´¥ã€‚")
            logger.info(f"   [INTL] æˆåŠŸè·å–ä»»åŠ¡ ID: {task_id}")
            for i in range(120):
                await asyncio.sleep(3)
                status_url = task_status_url_template.format(task_id=task_id)
                status_response = await client.get(status_url, headers=headers)
                if status_response.status_code == 200:
                    data = status_response.json()
                    if data.get("status") == "succeeded":
                        logger.info(f"   [INTL] {model_name} ä»»åŠ¡æˆåŠŸï¼")
                        return self._format_media_response(data, request_data, model_name)
                    if data.get("status") == "failed":
                        raise RuntimeError(f"ä»»åŠ¡å¤±è´¥: {data.get('result', 'æœªçŸ¥é”™è¯¯')}")
            raise TimeoutError("ä»»åŠ¡è¶…æ—¶ã€‚")

    def _format_media_response(self, task_result: Dict[str, Any], request_data: Dict[str, Any], task_type: str) -> JSONResponse:
        model_name = request_data.get("model")
        items = task_result.get("result", {}).get("images" if "wanx" in task_type else "videos", [])
        urls = [item.get("url") for item in items if item.get("url")]
        content = "\n".join(f"!image({url})" for url in urls) if "wanx" in task_type else "\n".join(f"è§†é¢‘é“¾æ¥: {url}" for url in urls)
        response_data = {"id": f"chatcmpl-{uuid.uuid4().hex}", "object": "chat.completion", "created": int(time.time()), "model": model_name, "choices": [{"index": 0, "message": {"role": "assistant", "content": content or "ç”Ÿæˆå®Œæˆï¼Œä½†æœªèƒ½è·å–é“¾æ¥ã€‚"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}}
        return JSONResponse(content=response_data)